# laBelup_crawler 프로젝트


## 동작을 위해 필요한 파일

1. teniron.json -> 각 사이트의 api 또는 해당 url과 해당 사이트에서 발급받은 apikey를 모아둔 json 파일.(github에 올릴거라 따로 생성? 등록기를 만들수도)
2. iplist가 들어간 파일 -> 추후엔 서버를 만들어 받아올 목록 예시라고 보면 된다.

## 우선 사항 

- 메뉴 형식으로 기능을 정의하여 다양한 API를 사용할 수 있도록 하기.

## 개발 레벨 정하기?

- 1단계 기능 완성
    - test1
- 2단계 모듈화 
    - test2
- 3단계 코드 효율성 증대.
    - test3

## 진행사항

<list>
    <ul>
        ~09/01 API를 이용할지 Selenium을 이용할지 기본 툴 확정 및 기초공사 끝.
    </ul>
    <ul>
        Nerd는 현재 API를 사용하려면 계정이 필요 -> 계정은 관리자를 통해서만 만들 수 있음. -> 메일로 계정 생성 요청해 둠.
    </ul>
    <ul>
        09/02 NERD 계정 확보 완료 및 API를 통한 크롤링 테스트중
    </ul>
    <ul>
        09/12 문자열사이에서 ipv4만 뽑아내는 추출기 완성
    </ul>
    <ul>- server error시
        <ul> 방법 1. 바로 재시도
            <ol>1. 서버 에러이므로 5초 후 재시도, 3회 재시도 후 안되면 종료, 재시도 시 로그 남기기.</ol>
            <ol>2. 저장이 필요한 데이터 : err_n, error, ip. => 로그에 저장</ol>
        </ul>
        <ul> 방법 2. 모아서 나중에 재시도 (특정 사이트에 적용)
            <ol>1. 에러 발생시 ip만 그냥 모아둠.</ol>
            <ol>2. 끝까지 진행후 다시 재시도(sleep 1)</ol>
        </ul>
    </ul>
</list>

## 크롤러 계획

<ul>1. API 이용 가능한 사이트 확인 -> ip 1개 대상으로 나오는 response값 형식 확인</ul>


<ul>2. API 이용이 불가능한 사이트 -> 직접 server로 query날려서 결과값 받아보기</ul>
<ul>3. API 이용 불가능, query도 안먹는 사이트 -> 직접 selenium을 사용하여 크롤링 -> 많은 검색에 대해 어떻게 진행할지 고민 후 진행</ul>
 <ul>-- 시간문제, 봇으로 인지 안되도록</ul>

## DB 계획

위 크롤러들이 모아온 정보를 어떤 형식으로 어떻게 저장할지 정하기.
일단 모든 데이터에 대한 형식을 모두 받고, 사이트별로 DB table을 만드는 것으로 정해둠.

